# RAG Chatbot

A **Retrieval-Augmented Generation (RAG) Chatbot** that allows users to upload PDF documents and ask questions. The chatbot provides **context-aware answers** by combining a **LangChain RAG pipeline**, **FAISS vector database**, and a **local LLM as (Llama 3.1 via Ollama)**.

---

## Table of Contents

- [Project Overview](#project-overview)  
- [Architecture](#architecture)  
- [Project Structure](#project-structure)  
- [Features](#features)  
- [Installation](#installation)  
  - [Python Environment](#python-environment)  
  - [Docker Setup](#docker-setup)  
- [Usage](#usage)  
- [License](#license)  

---

## Project Overview

RAG Chatbot allows users to:

1. Upload PDF files through a web interface.  
2. Ask questions about the content of the uploaded PDFs.  
3. Receive accurate answers generated using a **local LLM** powered by a **LangChain RAG pipeline**.

The system integrates PDF processing, text chunking, vector embeddings, similarity search, and LLM-based answer generation.

---

## Architecture

<img width="250" height="400" alt="image" src="https://github.com/user-attachments/assets/9d66d08c-4ecc-4846-9b1c-ec97cb77eaf6" />



---

## Project Structure

```

rag-chatbot/
â”‚ .gitignore
â”‚ Dockerfile
â”‚ LICENSE
â”‚ README.md
â”‚
â”œâ”€â”€â”€backend
â”‚ â”‚ config.py
â”‚ â”‚ main.py
â”‚ â”‚ requirements.txt
â”‚ â”‚
â”‚ â”œâ”€â”€â”€data
â”‚ â”‚ â””â”€â”€â”€faiss_index
â”‚ â”‚ index.faiss
â”‚ â”‚ metadata.json
â”‚ â”‚
â”‚ â”œâ”€â”€â”€rag
â”‚ â”‚ embeddings.py
â”‚ â”‚ llm.py
â”‚ â”‚ loader.py
â”‚ â”‚ pipeline.py
â”‚ â”‚ retriever.py
â”‚ â”‚ splitter.py
â”‚ â”‚ vectordb.py
â”‚ â”‚ init.py
â”‚ â”‚
â”‚ â””â”€â”€â”€uploads
â”‚
â””â”€â”€â”€frontend
index.html
script.js
style.css

```

## Features

- ğŸ”— **LangChain** â€” Implements the RAG pipeline  
- âš¡ **FastAPI** â€” Backend server for handling API requests  
- ğŸ¨ **HTML / CSS / JavaScript** â€” Frontend for file upload and Q&A  
- ğŸ§  **MiniLM Embeddings** â€” Embedding model for vector representation  
- ğŸ“Š **FAISS** â€” Vector database for similarity search  
- ğŸ¤– **Llama 3.1 (Ollama)** â€” Local LLM for generating answers  

### API Endpoints
- ğŸ“¤ `/upload` â€” Upload PDF files  
- â“ `/ask` â€” Ask questions about uploaded PDFs  

- ğŸ³ **Docker Support** â€” Containerized deployment


## Installation

### Python Environment

1. Create and activate a virtual environment:

```bash
python -m venv venv
venv\Scripts\activate     # Windows
```

2. Install dependencies:

```bash
 pip install -r backend/requirements.txt
```

### Docker Setup

1. Build Docker image:
```bash
docker build -t rag-chatbot .
```

2.Run Docker container:
```bash
docker run -it -p 8000:8000 rag-chatbot
```

Backend available at http://localhost:8000.

Open frontend/index.html to upload PDFs and ask questions.


## Usage

Upload your PDF via the frontend.

Ask your question.

Backend searches FAISS for relevant chunks and generates an answer using Llama 3.1.

Receive context-aware answers.

## License

This project is licensed under the **MIT** License â€” free to use, modify, and distribute.









